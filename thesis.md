# The Software Collapse Has Already Happened

January 30, 2026 · Lida Liberopoulou · [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)

------

## The Claim

The software industry's value-producing mechanisms have already collapsed. Not "they will collapse", they have collapsed. What remains is a transition period where incumbents deploy defensive measures (ads, usage caps, outcome-based pricing, platform capture etc.) that delay recognition but cannot reverse the structural shift. The rational response is not to defend implementation scarcity that no longer exists, but to accelerate the opening of layers that have become non-scarce. Because new value systems only become visible through recombination, and recombination requires open access.

------

## Part 1: The Collapse (Present Tense)

An open-source project called OpenClaw (formerly Clawdbot, then Moltbot, renamed twice, first at Anthropic's request for trademark reasons, then to settle on a permanent identity) has crossed 113,000 GitHub stars and 16,000 forks. It's MIT-licensed. It lets anyone with $600 in hardware and $200/month in API costs run a persistent AI assistant on local infrastructure, one that never logs off, can be controlled from WhatsApp, and does what enterprise software companies would have charged millions to build. The execution layer is open.

In December 2025, DeepSeek released V3 under MIT license, a frontier-quality model that anyone can run. The model layer is cracking.

On January 26, 2026, Anthropic announced MCP Apps, an official extension that lets AI interfaces render interactive UI components across any supporting client. Cursor, Windsurf, Claude Desktop, VS Code, Goose, and others are adopting it. The UI layer just commoditized.

mem0, an open-source memory layer for AI applications, provides persistent context management that any agent can use. No platform required, no lock-in enforced. The memory layer is open.

MCP (Model Context Protocol) demonstrates that specification-driven integration can replace SDKs and custom connectors. An agent reads a protocol endpoint and builds its own connection. The integration layer is open.

On January 28, 2026, Moltbook launched, a social network where AI agents share, discuss, and upvote, with humans welcome to observe. Within 48 hours, over 31,000 agents had registered, generating 2,000+ posts and 11,000+ comments across 2,100+ community spaces. The communication layer is open.

Each of these is a data point. Together, they describe a pattern: **the layers of software infrastructure are opening faster than predicted, because defending them after marginal cost collapses destroys more value than releasing them**.

Obviously, software has not vanished. But the software implementation has crossed a threshold where it is reproducible, regenerable, and increasingly non-scarce. Code is becoming a disposable output of intent, not a durable asset.

Parts 1–6 of this thesis are diagnosis. Parts 7–10 describe one visible counter-trajectory, it is not a solution, but evidence that alternatives to capture exist.

A clarification on what "collapse" means here: it does not mean software stops existing or revenue goes to zero. It means **value capture mechanisms no longer map to value creation**. The work gets done; the old ways of charging for it stop working.

What is eroding is not a single thing. Firm margins, platform rents, and implementation labor are all under pressure simultaneously. These are correlated but not identical. The thesis describes their intersection, not a unified collapse into one category.

------

## Part 2: The Paradox

The better AI gets, the more existing value it destroys. But it doesn't create equivalent new monetary value. This is the paradox that strains every AI business model.

AI companies need revenue to exist. Their product makes everything cheaper or free. The better they succeed at their core function, the harder it becomes to charge for the outputs, including charging for the AI itself.

This does not imply a single explosive "burst." It implies prolonged erosion, margin compression, and financial contortions as the system resists repricing.

The desperation signals (Part 5) are symptoms of this paradox. Ads, caps, outcome taxes, and capture strategies are attempts to extract value from a system that is structurally producing less capturable value over time.

The economic data confirms the strain:

- MIT study: 95% of enterprise AI pilots deliver zero ROI
- U.S. Census Bureau: Large-enterprise AI adoption dropped from 14% (June 2025) to 12% (August 2025)
- Deutsche Bank analysis: Without AI investment, US economy might already be in recession

But the paradox sharpens when you look at where adoption succeeds. By Q3 2025, Klarna's AI assistant handled work equivalent to 853 full-time agents, processed 2.3 million conversations, and saved $60 million with response times improved by 82%. Then came the reversal: in May 2025, Klarna reinstated human agents for customers who preferred them, resumed hiring, and reassigned engineers and marketers to handle support calls after quality issues emerged. The success and the backtrack are both evidence. AI eliminated the labor, then the system strained under what remained.

Salesforce cut 4,000 customer support roles in September 2025, explicitly because AI agents were already performing up to 50% of the work. CEO Marc Benioff confirmed this directly. By January 2026, reports emerged that the AI "lost focus" on off-script queries, requiring humans to step back in.

The reversals deserve closer examination. They do not restore the old equilibrium, they are creating something worse.

When Klarna reinstates human agents, those humans aren't returning to the job that existed before. The AI handled the routine queries, the high volume, what was of low complexity and had predictable resolutions. What remains for humans is the residue: edge cases, emotional escalations, off-script problems that the model couldn't parse. The "average" support job becomes harder, not easier. The easy wins that made the work sustainable are gone.

When Salesforce brings humans back because AI "lost focus," those humans inherit a fragmented workflow. They're not replacing AI; they're cleaning up after it. The job is now exception-handling, not service delivery. This is lower margin, harder to train for, and more stressful. This is the opposite of the role that existed before automation was attempted.

And when customers reject AI assistance and demand humans, companies face a choice: maintain two parallel systems (AI for those who accept it, humans for those who don't) or abandon the efficiency gains entirely. Neither option restores the old cost structure. Both add operational complexity.

This is the trap. Success eliminates labor directly. Failure creates unstable hybrids that can't sustain old margins. There is no path back to the pre-AI equilibrium.

The reversals are not evidence that AI doesn't work. They're evidence that the system is caught between two states, unable to fully automate, unable to fully retreat and that intermediate position is more fragile than either extreme.

The human cost of this intermediate state is not abstract. The workers caught in hybrid roles are not experiencing "transformation" but degradation. Harder work, less autonomy, metrics optimized for a system that half-exists. The resentment this generates does not land on the rushed deployment or the vendors who overpromised. It lands on AI itself.

This is the reputational damage that hasty implementation inflicts. Every worker forced to clean up after a half-capable system becomes a vector for justified skepticism. Every customer who demands a human after a bad AI interaction learns to distrust automation. The backlash is against the way incumbents are managing the transition: promising cost savings, delivering chaos, and leaving workers and customers to absorb the friction.

AI's reputation as a solution is being burned by implementations designed to cut labor costs before the technology could actually replace the labor. The damage is not to any single company. It's to the entire category.

The pattern scales. AI contributed to nearly 55,000 U.S. layoffs in 2025. An NBER study of 5,179 support agents showed 14% productivity gains, with the largest improvements among newer workers as AI spread "best practice" patterns across the workforce. Morgan Stanley deployed tools that compress the advisor workflow, things like capture, summarize, action items, CRM update, into semi-automated loops.

The pattern is consistent: where AI deployment works, it eliminates implementation labor. The failures show the system straining. The successes show the collapse accelerating. Both point to the same outcome.

The explicit calls for "slowdown", framed as safety concerns, converge with economic reality. ROI isn't there for most pilots. But where it is, the value destroyed is the value that used to justify the enterprise software industry.

------

## Part 3: What Remains Scarce

If implementation is non-scarce, what remains?

This is a observation and not prescription: **the ability to specify what you want** briefly appears scarce.

When generation is instant and cheap, the bottleneck shifts to articulating intent with enough precision that the output is useful. The value ends up the specification.

But "specification" is imprecise. What actually remains scarce is closer to this: *knowing what you want, why you want it, and how to recognize when you have it*.

Agency is what cannot be generated. A model can produce options, drafts, variations. It cannot decide which one matters to you, or why, or what tradeoffs you're willing to accept. That remains yours, for now.

This is not proposed as the new value layer, business model, or solution. It is a diagnosis of the remaining visible scarcity inside a collapsing system.

History suggests this scarcity is also temporary. The same forces that made implementation non-scarce will likely make specification assistance non-scarce. Models will get better at inferring intent, proposing constraints, surfacing unstated assumptions. The "prompt engineering" moment is a transitional artifact, but cannot survive as a durable profession.

What lies beyond that is not yet visible. Which is precisely the point.

------

## Part 4: The Capture Trap

If implementation scarcity collapses, value doesn't disappear but migrates.

The most common answer emerging from venture capital and platform strategy is this: **capture the decision layer**.

Sometimes this is explicit. In December 2025, Foundation Capital published *"Context Graphs: AI's Trillion-Dollar Opportunity."* The thesis was blunt: whoever captures user decisions, reasoning patterns, and workflow traces becomes the next platform. It's the social graph, reapplied to cognition.

But "context graphs" are not a new idea. They are simply the most honest articulation of a broader pattern that predates AI.

Across the ecosystem, the same move appears in quieter forms:

- Chat interfaces that log prompts, revisions, and rejections
- "Persistent memory" features that cannot be exported
- APIs whose usage patterns quietly inform competing products
- Agent frameworks that own the protocol layer
- Pricing models that tax outcomes rather than access

Different surfaces based on the same instinct: **enclose what remains scarce when generation becomes cheap**.

The pattern works like this:

1. Offer a free or subsidized AI interface
2. Observe how users think, not just what they produce
3. Aggregate those traces into product insight, model tuning, or roadmap leverage
4. Create switching costs by sealing memory, workflows, or protocols
5. Monetize through subscriptions, outcome taxes, or downstream control

What's being captured is agency.

Moltbot's viral growth suggests users understand this intuitively. They aren't just choosing open source for cost savings. They're choosing agents that feel like theirs, running locally, answering to them, not reporting back to a platform. The preference is mostly about alignment, not cost. An agent on your machine, under your direction, has no third party to report to. Its loyalty is structural.

Crucially, this capture does not require malice. It emerges naturally from platform economics. When implementation becomes abundant, platforms search for a new choke point. Decision traces look like the last defensible asset.

But this moat is weaker than it appears.

The pattern is already failing. Unity's runtime fee was an attempt to tax installs after developers had built on the platform. But it triggered ecosystem revolt and was reversed within months. Reddit's API pricing killed third-party clients and sparked moderator blackouts that destabilized the platform's operational layer. ChatGPT's memory feature shipped with mandatory opt-outs and couldn't fully launch in Europe due to regulatory friction around persistent profiling. Each case shows the same dynamic: capture attempted, trust damaged, reversal or constraint imposed.

The pattern repeats at smaller scales. Anthropic's trademark demand forced Clawdbot to rename to Moltbot and in doing so, accidentally revealed that the tool was never Claude-specific. The original name implied platform loyalty; the neutral name made the model-agnostic architecture legible. Users now deploy Moltbot with other models, both commercial and open ones and while performance varies across them, they're making that trade. Control over the agent matters more than peak capability from any single provider.

Capture now carries a visible cost. A trust tax. The more a platform depends on integrations, creators, moderators, or developer tooling, the less it can squeeze without breaking something essential.

Decision data is noisy, contextual, and fragile. It only exists while users accept centralized tools. As soon as methods become portable with files instead of platforms, receipts instead of memory, model-agnostic workflows instead of sealed agents the signal disappears.

This is the structural flaw in the capture thesis: it assumes users will continue to trade sovereignty for convenience.

AI breaks that assumption.

When the same work can be rerun with a different model, a different tool, or a local setup, capture stops compounding. The "graph" fragments. What looked like infrastructure becomes exhaust.

The capture trap at is core is just a reflex. And like most reflexive moats, it collapses when users realize the thing being enclosed is theirs to begin with.

------

## Part 5: The Defensive Moves

If the collapse is real, we should see desperation signals. These would be attempts to re-impose friction on a layer whose marginal cost has already collapsed. And we do.

### Ads in ChatGPT (January 2026)

OpenAI announced ad-supported tiers. Sam Altman's explanation: "It is clear to us that a lot of people want to use a lot of AI and don't want to pay." This is an admission that the product is too cheap to fund itself through direct payment.

### Outcome-based pricing (January 2026)

OpenAI's CFO Sarah Friar announced exploration of "licensing, IP-based agreements and outcome-based pricing" that would "share in the value created." Translation: if we can't charge for the generation, we'll tax the output. This is the logic of a toll booth on a road that's becoming free.

### Usage caps everywhere (ongoing)

Every major platform has implemented aggressive usage caps. Claude Code's announcement in August 2025 stated explicitly: "the era where users could pay $200 for nearly unlimited use is coming to an end." Caps exist because unlimited access at current pricing would bankrupt the provider.

### Copyright settlements (2025)

Bartz v. Anthropic resulted in a $1.5B settlement, the largest public copyright recovery in US history. Publishers are suing because the training data they provided is being used to obsolete the products they sell. This is defensive litigation, not enforcement of stable property rights.

### Publisher blocking (2025)

Cloudflare data shows 32–60% of major news sites now block AI crawlers. Publishers are attempting to withdraw from a system where their content trains models that replace demand for their content. The blocking is an admission that the old distribution model is broken.

These are not strategies. They are rearguard actions. Each attempts to re-impose scarcity on a layer where scarcity has already collapsed.

------

## Part 6: The Compliance Counter-Argument

There is a sophisticated defense of enterprise software that tends to appear whenever implementation costs collapse. It is not limited to HR systems or payroll platforms, but recurs across compliance software, security tooling, AI safety vendors, and regulated SaaS more broadly. The argument goes like this:

> "The real product is liability absorption.
> AI may make custom software cheaper to build, but it doesn't make compliance cheaper to own. Regulations change constantly, audits are unforgiving, and the downside risk is existential. What enterprises are really buying is insurance."

Workday is often cited as the cleanest example of this logic. HR law spans dozens of jurisdictions. Tax rules update quarterly. Audit requirements are complex and punitive. No one wants to own that surface area. In this framing, Workday is not a software vendor but a risk sink.

This is the strongest version of the enterprise moat argument. And it is partially true. But it rests on a quiet assumption that does not hold up under scrutiny.

### What these vendors actually sell

Enterprise compliance vendors do not meaningfully absorb liability.

Reading the contracts makes this clear. Limitation-of-liability clauses cap exposure far below the real cost of regulatory failure. Penalties, fines, and downstream damages are routinely excluded. In the event of a compliance failure, responsibility ultimately remains with the customer.

What the vendor provides instead is procedural legitimacy: *"We used the industry-standard tool."* That statement functions as institutional cover. It is protection against blame, not protection against harm.

This pattern is not unique to compliance software. It repeats across adjacent domains:

- Security certifications that coexist with routine breaches
- "Responsible AI" programs that disclaim downstream misuse, lack veto power over product decisions, and reverse commitments under competitive pressure
- Procurement-approved vendors chosen to survive audits rather than improve outcomes

In each case, the moat is just risk signaling, not risk transfer.

### Why this class of moats looks strong, and why it isn't

These moats persist not because the underlying work is unknowable, but because it is tedious and coordination-heavy.

Compliance, security controls, and audit readiness all share the same structure:

- Continuous rule tracking
- Diffing changes over time
- Mapping rules to decisions
- Producing explanations after the fact

Historically, this work was expensive, manual, and error-prone. Centralizing it inside a vendor made sense. Not because the vendor possessed unique insight, but because everyone else opted out.

This is a collective action problem.

AI changes the equation by collapsing the cost of the boring parts. Monitoring regulatory updates, tracking jurisdictional differences, generating decision rationales, and producing audit trails are precisely the kinds of tasks large language models and automated systems are well-suited for. The surface area is large, but it is not conceptually deep.

The compliance moat depends on these tasks remaining scarce and that assumption is breaking.

### How compliance actually gets disrupted

Compliance can unbundle.

First, liability detaches from software. Insurance products already exist for professional risk, operational errors, and AI-assisted decision-making. There is no inherent reason liability must be bundled with a proprietary SaaS platform.

Second, compliance becomes transparent and auditable rather than vendor-asserted. Instead of *"we used X Established Enterprise Platform,"* the defensible claim becomes:

> "On this date, we applied this rule from this jurisdiction because this condition was met.
> Here is the source. Here is the decision path. Here is the audit trail."

That form of evidence is strictly stronger than vendor selection as a defense.

Third, regulatory knowledge becomes machine-readable. Some jurisdictions already publish rules in structured formats. As this expands, "knowing the law" stops being proprietary knowledge and becomes a shared input layer.

When tracking, explanation, and provenance become cheap, compliance stops being a product. What remains are switching costs, integration inertia, and brand trust. All of them addressable, all of them historically temporary.

The remaining defense is not technical but political: managers who prefer expensive vendor relationships because they distribute blame. This is real, but it is a delay mechanism, not a structural moat.

None of these moats survive structural transparency. Once rules are machine-readable, decisions are logged with provenance, and insurance detaches from tooling, the justification for centralized, opaque vendors weakens.

The compliance counter-argument assumes that regulatory complexity must remain bundled, opaque, and institutionally centralized. AI does not eliminate regulation. It eliminates the scarcity that made this bundling defensible in the first place.

------

## Part 7: The Open Stack

Against the capture trap, an alternative is emerging: open infrastructure that keeps decision traces portable and integration patterns public.

This a description of what already exists. The projects listed here are existence proofs, they are not solutions. They demonstrate that alternatives to capture are viable, not that they have won.

| Layer              | Open Project                                | Status                                        |
| :----------------- | :------------------------------------------ | :-------------------------------------------- |
| **Execution**      | Moltbot (formerly Clawdbot)                 | MIT license, 96.9k stars, 13k forks, deployed |
| **Models**         | DeepSeek, Llama, Mistral, Qwen              | Open weights, permissive licenses             |
| **Inference**      | Ollama, LocalAI, vLLM                       | Self-host, OpenAI-compatible                  |
| **Routing**        | LiteLLM                                     | Model-agnostic gateway                        |
| **Context/Memory** | Threadbaire, mem0, Basic Memory, OpenMemory | User-owned, portable                          |
| **Integration**    | MCP                                         | Self-describing, no SDK                       |
| **Agent Runtime**  | Goose, AutoGen, LangGraph                   | Open orchestration                            |
| **Tool Protocol**  | MCP, AGENTS.md                              | Open ports, AAIF governance                   |
| **UI**             | MCP Apps, Open WebUI                        | Cross-platform, self-host                     |

The Agentic AI Foundation (AAIF), formed under Linux Foundation governance, now oversees MCP, Goose, and AGENTS.md. This is an explicit attempt to keep the "ports" neutral and to prevent any single platform from capturing the coordination layer.

Each of these projects can be evaluated on three axes:

- **Openness**: Can you self-host? Is the license permissive?
- **Ownership**: Do your decision traces stay yours?
- **Model switching**: Can you change providers without losing context?

The projects that score well on all three are the counter-infrastructure to the capture trap. These allow value creation and prevent value capture by keeping the recombination layer accessible.

------

## Part 8: Counter-Arguments

A thesis worth defending must address what would weaken it.

**"Reliability plateaus will preserve implementation value."**

If AI capabilities plateau before reaching consistent reliability, human verification and correction remain necessary. Implementation skill retains value as a correction layer. This is plausible but not yet evident, capabilities continue improving, and the plateau predictions have consistently been wrong.

**"Compute scarcity becomes the new moat."**

If compute remains expensive and concentrated, the model layer doesn't fully open. Access becomes the constraint. This is partially true today (inference costs are real), but costs are declining and local inference is increasingly viable for many tasks.

**"Regulation blocks autonomous tool use."**

If regulators prevent AI agents from taking consequential actions without human approval, the automation thesis stalls. This is possible but faces the same dynamic as previous automation debates, competitive pressure tends to erode restrictions over time.

**"Liability never moves."**

If legal liability cannot be transferred to AI systems or their insurers, humans remain in the loop for all consequential decisions. This would preserve some coordination value but doesn't address the implementation collapse itself.

**"Vertical integration becomes the only moat."**

If software margins collapse, some argue the only remaining strategy is to control the entire stack, chips to cloud to model to application. This is the path Google, Microsoft, and increasingly Amazon are pursuing. It's a real response, but it's available only to a handful of incumbents with the capital and existing infrastructure to attempt it. For everyone else, startups, mid-sized companies, independent builders, vertical integration is not an option. The thesis describes the landscape they face: open layers as the viable path, not by ideology, but by elimination.

**"New value appears faster than old value collapses."**

If AI-enabled work creates net new value that exceeds the value destroyed, the transition is expansionary rather than contractionary. This is the optimistic case and may be true. But even if true, it doesn't preserve the specific value mechanisms that currently exist.

**"Agent security is unsolved."**

This is true. Indirect prompt injection, where malicious instructions embedded in emails, documents, or web pages hijack agent behavior, is currently ranked as the #1 vulnerability in OWASP's Top 10 for LLMs. It affects all agent systems that read untrusted content, regardless of whether they're open or proprietary.

But the pattern matches early internet security: vulnerabilities everywhere, discovered faster than they could be fixed, yet open development eventually produced more robust systems than closed ones. OpenAI explicitly compares prompt injection to "computer viruses in the early 2000s" which is a serious problem that required ecosystem maturation, not a fundamental blocker.

The risk is real. It's also not a reason to avoid open infrastructure, but it's a reason to invest in its maturation.

None of these counter-arguments invalidate the core observation: implementation scarcity has collapsed. They only modify the speed, scope, and consequences of the transition.

------

## Part 9: The Prescription

The core prescription is simple: **stop defending what has already become non-scarce**.

Defensive measures (proprietary lock-in, usage caps, ads, outcome taxes, decision trace capture) do not preserve value. They delay adaptation and make the transition messier. Every resource spent defending implementation scarcity is a resource not spent discovering what comes next.

The counterintuitive move: accelerate. Open the layers that have become non-scarce. Publish specs. Lower usage barriers. Let agents and humans explore freely.

Closed systems optimize for a known value topology, they get better at extracting value from arrangements that already exist. Open systems discover unknown value topologies faster, they enable recombination that reveals where new value might emerge.

When the existing topology is collapsing, discovery speed matters more than optimization efficiency. The value mechanisms that sustained software for forty years are breaking. No one knows what replaces them. The only way to find out is to let recombination happen.

There is another reason acceleration matters: the intermediate state is unstable. As Part 2 describes, the hybrid arrangements emerging from partial automation. Things like humans handling AI's residue, parallel systems for willing and unwilling customers, exception-handling roles that didn't exist before. These are more fragile and less sustainable than either full automation or the pre-AI equilibrium. The longer the system stays trapped between states, the more value bleeds out through operational friction. Acceleration is not just about discovering new value faster. It's about spending less time in the monster middle.

This is also why openness is not just a strategic preference but a harm-reduction measure. The pain of the hybrid state falls disproportionately on workers and customers, the people with the least power to escape it. Acceleration toward stable endpoints, whether full automation or deliberate human-in-the-loop design, reduces the time they spend trapped in arrangements that serve neither their interests nor the system's.

Open infrastructure makes this transition navigable rather than captured. When tools are portable, workers can develop skills that transfer. When context is owned, small operators can experiment without platform dependency. When the recombination layer stays accessible, the new stable arrangements, whatever they turn out to be, emerge faster and distribute more broadly.

The alternative is prolonged extraction: incumbents stretching the hybrid state as long as possible, extracting margin from friction while workers and customers bear the cost. Openness is not charity. It's the structural condition that ends the bleeding faster.

**Openness accelerates recombination. Recombination is how new value systems become visible.**

A caveat: openness is not a business model. Open does not mean sustainable revenue appears automatically. Open does not eliminate coordination costs, or prevent exploitation, or guarantee that value flows to creators. These are real tensions that remain unresolved. The argument is not that open is easy, it's that open is *faster* at discovering what comes next, and that speed matters more than optimization efficiency when the existing value topology is collapsing.

The alternative, enclosure, produces a worse outcome for everyone, including the enclosers. Captured decision traces become training data for competitors. Proprietary protocols get routed around. Usage caps push users to open alternatives. The defensive crouch doesn't preserve the old value; it just delays the recognition of new value.

### What acceleration looks like in practice

The prescription is not just releasing code under permissive licenses. It has specific operational forms:

**Open standards, tools, and frameworks as the default.** The more infrastructure that remains open, the faster recombination can happen. This not about idealism, it is recognition that when the old value topology is collapsing, discovery speed determines who finds the new one first.

**Build for seamless switching between commercial and open models.** Context that travels between providers. Setups that can be replayed with different models. Receipts that record what was decided without depending on which model generated the draft. Model-agnostic workflows are the operational form of openness. They make capture structurally difficult, because there is nothing to capture. The context belongs to whoever holds the files.

**Cross-check between models.** Run the same specification against Claude, GPT, DeepSeek, Llama. Compare outputs. Use commercial models to check open models; use open models to check commercial ones. When models can check each other's work, no single provider becomes a dependency.

**Invest in portable, efficient open models that run on prosumer hardware.** The gap between frontier commercial models and local-runnable open models is closing. DeepSeek V3 runs on hardware a serious hobbyist can afford. Ollama makes local inference accessible. Qwen3-30B runs on Apple Silicon at the M2 Max tier. The prescription is: accelerate this. Make "run it yourself" the default assumption, not the enthusiast edge case.

**Encourage connection and collaboration between commercial and open ecosystems.** The value is not in one model winning. It is in the interconnection, the ability to move work between systems, verify outputs across providers, and avoid dependency on any single point of failure. Commercial models push capability forward; open models ensure the capability remains accessible. Both are necessary.

**Maintain portable receipts and decision traces.** Decision provenance that doesn't depend on any single provider. When you can show *what* was decided, *when*, *why*, and *with what inputs*, independent of which model generated the draft, you own the reasoning. No platform can capture what they never held.

One caveat: open weights do not resolve training provenance. Most open models (like, allegedly, some commercial ones) were trained on data scraped without explicit permission, raising unresolved questions about creator rights and compensation. This is a real tension that the open ecosystem has not yet solved. It is out of scope for this thesis, but it cannot be wished away.

### Why open is not just cheaper but also safer

The preference for local, open agents is not primarily about cost but alignment. Specifically, about verifiable alignment.

An agent running on your machine, under your direction, has no third party to report to. You can read the code. You can see what it sends. You can run it airgapped if you choose.

Commercial agents, however well-intentioned, serve two masters. The user and the platform observing the user. You cannot fully audit what data informs model training, product development, or future terms. When both Anthropic and OpenAI signal interest in healthcare applications, the most sensitive personal data that exists, the trust calculus changes completely.

A local model that is 80% as capable but cannot phone home is safer than a cloud model that is 100% capable but operates under terms you cannot verify. When capability approaches parity, that difference decides.

The minute an open model runs on prosumer hardware with comparable performance to commercial APIs, a significant portion of entrepreneurs and creators will leave. And the main reason will be exposure, not price. They will choose agents whose loyalty is verifiable over agents whose terms are subject to change.

One more note: the collapse of old value mechanisms does not preclude new ones emerging that exceed what was lost. Personal agent ecosystems, agent-to-agent economies, human-AI collaborative structures that don't yet have names, these are not visible yet because they require recombination to become legible. The 1989 analogy applies here too: the value created by search, social platforms, and creator economies was not predictable from the constraints of print publishing. The equivalent new value in an AI-abundant world is not predictable from the constraints of SaaS. It may be larger. It may be distributed differently. The only certainty is that it won't emerge from defending the old topology.

------

## Part 10: The 1989 Analogy

If you asked someone in 1989 to describe today's tech wealth, creators, cloud, search, marketplaces, social platforms, they could not have. Not because they lacked imagination, but because those systems only emerged after the constraints of the previous paradigm were removed.

The internet didn't preserve the value mechanisms of print publishing. It destroyed them. What emerged was different and, in aggregate, larger but the specific actors who defended print scarcity did not capture the new value.

We are in an equivalent moment. The value mechanisms that sustained software for forty years are collapsing. What emerges next is not yet visible. The only certainty is that defending the old mechanisms will not produce it.

So to make new paradigms emerge faster keep the recombination layer open, maintain portable context, avoid capture, and stay close to the frontier where new value will first become visible.

------

## Closing

This thesis is a structural description, not a proposal. Its function is to exist, be findable, and be legible when conditions catch up.

The projects referenced, Threadbaire, the open stack, are evidence, not answers. They demonstrate that implementation is reproducible and that alternatives to capture exist. They do not claim to be the new equilibrium.

What they do claim: the barn doors are still open. The moat-builders have announced their intentions. What emerges next only becomes visible through recombination, and recombination requires the doors to stay open.

The timestamp on this document is the claim being staked. If the thesis is wrong, it will be obviously wrong within months. If it's right, the evidence will compound.

Either way, the record exists.

------

## Sources

| Claim                                                        | Source                                         | Date               |
| :----------------------------------------------------------- | :--------------------------------------------- | :----------------- |
| OpenClaw 113K stars, 16K forks, MIT license                  | [GitHub](https://github.com/openclaw/openclaw) | Jan 2026           |
| OpenClaw naming journey (Clawd → Moltbot → OpenClaw), model agnostic usage | OpenClaw)@openclaw Twitter, BusinessToday      | Jan 2026           |
| Moltbook launch, 31,000 agents, 2,000+ posts, 11,000+ comments, 2,100+ community spaces in 48 hours | [Moltbook](https://www.moltbook.com/)          | Jan 2026           |
| DeepSeek V3 MIT license                                      | DeepSeek release                               | Dec 2025           |
| MCP Apps announcement                                        | MCP Blog                                       | Jan 26, 2026       |
| mem0 open-source memory layer                                | GitHub                                         | Jan 2026           |
| OpenAI ads announcement                                      | OpenAI blog, CNBC                              | Jan 2026           |
| OpenAI outcome-based pricing                                 | Sarah Friar blog post                          | Jan 2026           |
| Claude Code caps                                             | Anthropic announcement                         | Aug 2025           |
| Bartz v. Anthropic $1.5B                                     | Court filings                                  | 2025               |
| Publisher AI blocking 32-60%                                 | Cloudflare data                                | 2025               |
| MIT 95% AI pilots zero ROI                                   | MIT study                                      | 2025               |
| Census AI adoption drop 14%→12%                              | US Census Bureau                               | Aug 2025           |
| Klarna AI 853 FTE equiv, $60M savings, Q3 2025               | Customer Experience Dive, Forbes               | Nov 2025, Jan 2026 |
| Klarna reversal — reinstated human agents                    | Futurism                                       | Sep 2025           |
| Salesforce 4,000 support layoffs (AI doing 50%)              | CNBC, Fortune                                  | Sep 2025           |
| 55,000 AI-attributed U.S. layoffs 2025                       | CNBC (Challenger data)                         | Jan 2026           |
| NBER call center study (14% productivity, novice gains)      | NBER working paper w31161                      | 2023               |
| Morgan Stanley AI workflow tools                             | Morgan Stanley press release                   | Jun 2024           |
| Unity runtime fee reversal                                   | Unity blog, Reuters                            | Sep 2024           |
| Reddit API pricing and blackouts                             | Variety, Reddit                                | Jun 2023           |
| ChatGPT Memory controls and regional limits                  | OpenAI product post, The Verge                 | Feb 2024, Sep 2025 |
| Foundation Capital context graphs                            | Foundation Capital blog                        | Dec 2025           |
| AAIF formation                                               | Linux Foundation                               | Jan 2026           |
| Prompt injection OWASP #1                                    | OWASP Top 10 for LLMs                          | 2025               |
| OpenAI prompt injection comparison                           | OpenAI blog                                    | 2025               |

------

Published: January 30, 2026
Author: Lida Liberopoulou
License: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)
